{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation of libraries and imports"
      ],
      "metadata": {
        "id": "vYsmvfpcg6OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive if files are uploaded to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF9zGD8DoE5o",
        "outputId": "a6776181-ecfa-4b54-9d36-36b5240e735c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlzdDneUgsct",
        "outputId": "cdad020b-e76a-4ef2-aacf-af60279dbd76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.0/260.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.7/58.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.3/362.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.3/202.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.6/91.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.6/341.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.2/397.2 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.9/179.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for logutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for simplegeneric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Dependencies\n",
        "!pip install torch torchvision transformers mistral mistralai grad-cam requests -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "from mistralai import Mistral\n",
        "import json\n",
        "import zipfile\n",
        "from torch.autograd import profiler\n",
        "import time\n",
        "import statistics\n",
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from torchvision.models import resnet50\n",
        "from torch import Tensor\n",
        "from torch.nn import Module\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "WhsAze2ihREz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Model Path and Input Zip folder"
      ],
      "metadata": {
        "id": "e-09p8fWm6eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the path to the model and the input zip file\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Adobe_model_CIFAKE_e50.pth\"  # Model path\n",
        "input_folder = \"/content/test-interiit.tar.gz\"\n",
        "\n",
        "# input_zip = \"/content/demo_test.zip\"\n",
        "# input_tar_gz = \"/content/test-interiit.tar.gz\""
      ],
      "metadata": {
        "id": "qWlGxA6HnDlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Classification of Image"
      ],
      "metadata": {
        "id": "848wBilChgb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):                    # Load Pretrained CLIP Model and Weights of Finetuned model\n",
        "    try:\n",
        "        state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
        "        device = torch.device('cpu')\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
        "        model.classifier = torch.nn.Linear(model.text_projection.in_features, 2).to(device)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "    except:\n",
        "        state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
        "        device = torch.device('cpu')\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "        model.classifier = torch.nn.Linear(model.text_projection.in_features, 2).to(device)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "    # print(model.eval())\n",
        "    return model"
      ],
      "metadata": {
        "id": "qUYti173he8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_mixed(image_path):             # Get and process image from image path\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    try:\n",
        "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "    except:\n",
        "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "JwZDh9QfhxfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):              # Get and preprocess image from image path (improvised)\n",
        "    # transform = transforms.Compose([\n",
        "    # transforms.Resize((224, 224)),\n",
        "    # transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "    #                      std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "    # ])\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "        ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    return transform(image).unsqueeze(0)"
      ],
      "metadata": {
        "id": "VPgz-Sdoh1wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(image_path, model):       # Wrapper Class for Task 1 to take input of image path and model and output whether it is Real or Fake\n",
        "    device = torch.device('cpu')\n",
        "    image_ten = preprocess_image(image_path).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(image_ten)\n",
        "        # classifier_head = torch.nn.Linear(model.config.projection_dim, 2)\n",
        "        logits = model.classifier(image_features)\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "        # print(f\"Predicted class: {predicted_class.item()}\")\n",
        "        # print(f\"Class probabilities: {probs}\")\n",
        "        return predicted_class.item()"
      ],
      "metadata": {
        "id": "jxTbKFR0iGHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - Explanability of Image Classification"
      ],
      "metadata": {
        "id": "a_w-ysU0iPRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):        # Encode Image to base64 to pass as an parameter to a Large Language Model\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {image_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "GMrpnRdZiON1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-Cam Mask of Image input\n",
        "\n",
        "class CLIPVisionWrapper(Module):\n",
        "    \"\"\"CLIP Vision Wrapper to use with Grad-CAM.\"\"\"\n",
        "\n",
        "    def __init__(self, clip_model: CLIPModel):\n",
        "        super().__init__()\n",
        "        self.clip_model = clip_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        outputs = self.clip_model.vision_model(x)\n",
        "        return outputs.pooler_output\n",
        "\n",
        "    @property\n",
        "    def target_layers(self):\n",
        "\n",
        "      layers = [\n",
        "          self.clip_model.vision_model.encoder.layers[-5].self_attn.q_proj,\n",
        "          self.clip_model.vision_model.encoder.layers[-4].self_attn.q_proj,\n",
        "          self.clip_model.vision_model.encoder.layers[-3].self_attn.q_proj,\n",
        "          self.clip_model.vision_model.encoder.layers[-2].self_attn.q_proj,\n",
        "          self.clip_model.vision_model.encoder.layers[-1].self_attn.q_proj\n",
        "      ]\n",
        "      return layers\n",
        "\n",
        "\n",
        "def grad_cam_clip(images: Tensor, clip_model: CLIPModel) -> Tensor:  #Performs Grad-CAM on a batch of images using CLIP's vision transformer\n",
        "    clip_model.eval()\n",
        "    clip_wrapper = CLIPVisionWrapper(clip_model)\n",
        "    cam = GradCAM(\n",
        "        model=clip_wrapper,\n",
        "        target_layers=clip_wrapper.target_layers,\n",
        "        reshape_transform=_reshape_transform\n",
        "    )\n",
        "    grayscale_cam = cam(\n",
        "        input_tensor=images,\n",
        "        targets=None,\n",
        "        eigen_smooth=True,\n",
        "        aug_smooth=True,\n",
        "    )\n",
        "\n",
        "    original_size = images.shape[2:]\n",
        "    grayscale_cam = torch.tensor(grayscale_cam).unsqueeze(1)\n",
        "\n",
        "    resized_cam = F.interpolate(\n",
        "        grayscale_cam,\n",
        "        size=original_size,\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    return resized_cam.squeeze(1)\n",
        "\n",
        "def _reshape_transform(tensor, height=14, width=14):    #Reshapes the output tensor to fit Grad-CAM's expected format\n",
        "    result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))\n",
        "    result = result.transpose(2, 3).transpose(1, 2)\n",
        "    return result\n",
        "\n",
        "def mask_gradcam(image_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_path=\"Adobe_model_CIFAKE_e50.pth\"\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "    clip_model.classifier = torch.nn.Linear(clip_model.text_projection.in_features, 2).to(device)\n",
        "    state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
        "\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "    img = Image.open(image_path)\n",
        "    resize_transform = transforms.Resize((224, 224))\n",
        "    img_resized = resize_transform(img)\n",
        "\n",
        "    inputs = processor(images=img_resized, return_tensors=\"pt\")\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    cam_output = grad_cam_clip(inputs[\"pixel_values\"], clip_model)\n",
        "    plt.imshow(cam_output[0].cpu().numpy(), cmap='jet', alpha=0.5)\n",
        "    plt.imshow(img_resized, alpha=0.5)\n",
        "    plt.axis('off')\n",
        "\n",
        "    file_name = img.filename.split('\\\\')[-1].split('.')[0]\n",
        "\n",
        "    # if not os.path.exists(\"gradcam_images\"):\n",
        "    #     os.makedirs(\"gradcam_images\")\n",
        "\n",
        "    output_path = f\"gradcam_images\\{file_name}_gradcam.jpg\"\n",
        "    print(output_path)\n",
        "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "    plt.close()\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "Mso1_4-l8-XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_explanations(base64_image, model_output, max_retries=3, retry_delay=5):\n",
        "    api_keys = [\"CjetUPdd9OZj42bO6ZnfCibWTMG9M90A\", \"sGLBkpRIs5LWLH9B4Dw3o0sXSlaghtvn\"]\n",
        "    class_name = \"AI Generated image\" if model_output == 1 else \"Real image and not AI generated\"\n",
        "    model = \"pixtral-12b-2409\"\n",
        "\n",
        "    artifact_list = f\"\"\"- Inconsistent object boundaries\n",
        "- Discontinuous surfaces\n",
        "- Non-manifold geometries in rigid structures\n",
        "- Floating or disconnected components\n",
        "- Asymmetric features in naturally symmetric objects\n",
        "- Misaligned bilateral elements in animal faces\n",
        "- Irregular proportions in mechanical components\n",
        "- Texture bleeding between adjacent regions\n",
        "- Texture repetition patterns\n",
        "- Over-smoothing of natural textures\n",
        "- Artificial noise patterns in uniform surfaces\n",
        "- Unrealistic specular highlights\n",
        "- Inconsistent material properties\n",
        "- Metallic surface artifacts\n",
        "- Dental anomalies in mammals\n",
        "- Anatomically incorrect paw structures\n",
        "- Improper fur direction flows\n",
        "- Unrealistic eye reflections\n",
        "- Misshapen ears or appendages\n",
        "- Impossible mechanical connections\n",
        "- Inconsistent scale of mechanical parts\n",
        "- Physically impossible structural elements\n",
        "- Inconsistent shadow directions\n",
        "- Multiple light source conflicts\n",
        "- Missing ambient occlusion\n",
        "- Incorrect reflection mapping\n",
        "- Incorrect perspective rendering\n",
        "- Scale inconsistencies within single objects\n",
        "- Spatial relationship errors\n",
        "- Depth perception anomalies\n",
        "- Over-sharpening artifacts\n",
        "- Aliasing along high-contrast edges\n",
        "- Blurred boundaries in fine details\n",
        "- Jagged edges in curved structures\n",
        "- Random noise patterns in detailed areas\n",
        "- Loss of fine detail in complex structures\n",
        "- Artificial enhancement artifacts\n",
        "- Incorrect wheel geometry\n",
        "- Implausible aerodynamic structures\n",
        "- Misaligned body panels\n",
        "- Impossible mechanical joints\n",
        "- Distorted window reflections\n",
        "- Anatomically impossible joint configurations\n",
        "- Unnatural pose artifacts\n",
        "- Biological asymmetry errors\n",
        "- Regular grid-like artifacts in textures\n",
        "- Repeated element patterns\n",
        "- Systematic color distribution anomalies\n",
        "- Frequency domain signatures\n",
        "- Color coherence breaks\n",
        "- Unnatural color transitions\n",
        "- Resolution inconsistencies within regions\n",
        "- Unnatural Lighting Gradients\n",
        "- Incorrect Skin Tones\n",
        "- Fake depth of field\n",
        "- Abruptly cut off objects\n",
        "- Glow or light bleed around object boundaries\n",
        "- Ghosting effects: Semi-transparent duplicates of elements\n",
        "- Cinematization Effects\n",
        "- Excessive sharpness in certain image regions\n",
        "- Artificial smoothness\n",
        "- Movie-poster like composition of ordinary scenes\n",
        "- Dramatic lighting that defies natural physics\n",
        "- Artificial depth of field in object presentation\n",
        "- Unnaturally glossy surfaces\n",
        "- Synthetic material appearance\n",
        "- Multiple inconsistent shadow sources\n",
        "- Exaggerated characteristic features\n",
        "- Impossible foreshortening in animal bodies\n",
        "- Scale inconsistencies within the same object class \"\"\"\n",
        "\n",
        "    prompt =  f\"\"\"Image is said to be an {class_name}. Explain on the following artifacts only(which are applicable) as to why it might be classified to be so(against or for).\n",
        "Do not output anything other than the explanations to the artifacts which are applicable. For each artifact, limit the explanations to 50 words.\n",
        "Do not format the text(no bold).\n",
        "Artifacts to be considered:\n",
        "{artifact_list} \"\"\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            api_key = api_keys[attempt % len(api_keys)]\n",
        "            client = Mistral(api_key=api_key)\n",
        "\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": f\"Image is said to be an {class_name}. Explain why based on visible artifacts.\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            chat_response = client.chat.complete(\n",
        "                model=model,\n",
        "                messages=messages\n",
        "            )\n",
        "\n",
        "            return chat_response.choices[0].message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                return f\"Failed to generate explanation after {max_retries} attempts: {str(e)}\""
      ],
      "metadata": {
        "id": "6uSR_B62B_Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jsonify(explanations):\n",
        "    api_key = \"MtZjdXzS1FPebXt18Y2BDQ9fhZQG3FDH\"\n",
        "    model = \"mistral-large-latest\"\n",
        "    client = Mistral(api_key=api_key)\n",
        "\n",
        "    prompt = f\"\"\"Convert the following explanations into a JSON object with artifact types as keys and their explanations as values. Format strictly as a valid JSON object.\n",
        "\n",
        "Content to convert:\n",
        "{explanations}\n",
        "\n",
        "Example output format:\n",
        "{{\n",
        "    \"blurred_edges\": \"Description of blurred edges\",\n",
        "    \"lighting_artifacts\": \"Description of lighting issues\"\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "- Use underscores instead of spaces in keys\n",
        "- Include only the JSON object, no additional text\n",
        "- Ensure all quotes are properly escaped\n",
        "- Each key-value pair should be an artifact and its explanation\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    response_text = chat_response.choices[0].message.content.strip()\n",
        "\n",
        "    # Clean up common formatting issues\n",
        "    response_text = response_text.replace('```json\\n', '').replace('\\n```', '')\n",
        "    response_text = response_text.strip()\n",
        "\n",
        "    try:\n",
        "        # First attempt: direct JSON parsing\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        try:\n",
        "            # Second attempt: Clean up potential formatting issues\n",
        "            cleaned_text = response_text.replace('\\n', ' ').replace('\\\\n', ' ')\n",
        "            cleaned_text = ' '.join(cleaned_text.split())  # Normalize whitespace\n",
        "            return json.loads(cleaned_text)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Failed to parse JSON. Raw response:\\n{response_text}\")\n",
        "            # Return a basic JSON object to avoid breaking the pipeline\n",
        "            return response_text"
      ],
      "metadata": {
        "id": "YrqK6seJi-Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Pipeline"
      ],
      "metadata": {
        "id": "7dfYrDFhEyBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(image_file, model, extracted_folder):\n",
        "    try:\n",
        "        image_path = os.path.join(extracted_folder, image_file)\n",
        "        model_output = predict_class(image_path, model)\n",
        "        masked_image_path = mask_gradcam(image_path)\n",
        "        base64_image = encode_image(masked_image_path)\n",
        "\n",
        "        explanation = gen_explanations(base64_image, model_output)\n",
        "        if \"Failed to generate explanation\" in explanation:\n",
        "            json_exp = {\"error\": explanation}\n",
        "        else:\n",
        "            json_exp = jsonify(explanation)\n",
        "\n",
        "        class_name = \"real\" if model_output == 0 else \"fake\"\n",
        "        index = int(image_file.split('.')[0])\n",
        "\n",
        "        return {\n",
        "            \"task1\": {\"index\": index, \"prediction\": class_name},\n",
        "            \"task2\": {\"index\": index, \"explanation\": json_exp}\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_file}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "v36OK0DmExvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "PpiKFfT7jbdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_time(model, image_path):  # Inference Time Calculator for Task 1 process\n",
        "\n",
        "    with profiler.profile(record_shapes=True) as prof:\n",
        "        with profiler.record_function(\"model_inference\"):\n",
        "            output = predict_class(image_path, model)\n",
        "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "    print()\n",
        "\n",
        "\n",
        "    for _ in range(10):\n",
        "        _ = predict_class(image_path, model)\n",
        "    num_iterations = 100\n",
        "    inference_times = []\n",
        "    for _ in range(num_iterations):\n",
        "        start_time = time.time()\n",
        "        output = predict_class(image_path, model)\n",
        "        end_time = time.time()\n",
        "        inference_times.append(end_time - start_time)\n",
        "    average_inference_time = statistics.mean(inference_times)\n",
        "    print(f\"Average inference time: {average_inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "O90QHubGk1au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN\n",
        "\n",
        "output_file_task1 = \"97_task1.json\"\n",
        "output_file_task2 = \"97_task2.json\"\n",
        "\n",
        "extracted_folder = input_folder.split(\".\")[0]\n",
        "extenstion = input_folder.split(\".\")[-1]\n",
        "\n",
        "if (extenstion == \"gz\"):\n",
        "    if not os.path.exists(extracted_folder):\n",
        "        with tarfile.open(input_folder, 'r:gz') as tar_ref:\n",
        "            tar_ref.extractall(extracted_folder)\n",
        "else:\n",
        "    extracted_folder = input_folder.split(\".\")[0]\n",
        "    if not os.path.exists(extracted_folder):\n",
        "        with zipfile.ZipFile(input_folder, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extracted_folder)\n",
        "\n",
        "\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Inference Time calculation\n",
        "#get_inference_time(model, extracted_folder + \"/demo_test/6.png\")\n",
        "print()\n",
        "\n",
        "# Run of Task 1 + Task 2\n",
        "results_task1 = []\n",
        "results_task2 = []\n",
        "\n",
        "subfolders = [f.path for f in os.scandir(extracted_folder) if f.is_dir()]\n",
        "if subfolders:\n",
        "    extracted_folder = subfolders[0]\n",
        "\n",
        "for image_file in os.listdir(extracted_folder):\n",
        "    if image_file.endswith((\".png\", \".JPEG\")):\n",
        "        result = run_pipeline(image_file, model, extracted_folder)\n",
        "\n",
        "        if result:\n",
        "            results_task1.append(result[\"task1\"])\n",
        "            results_task2.append(result[\"task2\"])\n",
        "            print(f\"Processed {image_file}:{result['task1']['prediction']}\")\n",
        "\n",
        "        time.sleep(2)  # Rate limiting\n",
        "\n",
        "with open(output_file_task1, \"w\") as json_file:\n",
        "    json.dump(results_task1, json_file, indent=2)\n",
        "print(f\"Results saved to {output_file_task1}\")\n",
        "with open(output_file_task2, \"w\") as json_file:\n",
        "    json.dump(results_task2, json_file, indent=2)\n",
        "print(f\"Results saved to {output_file_task2}\")"
      ],
      "metadata": {
        "id": "t0_URuAUjh8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}